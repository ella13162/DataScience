{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ella13162/DataScience/blob/main/ds_week_9_classification_seminar_student_version.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Lecture Content Flow**"
      ],
      "metadata": {
        "id": "mguWOZH5_Lh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **This lecture will classify the \"survived\" target variable of the \"titanic\" dataset and implement the following classification algorithms:**"
      ],
      "metadata": {
        "id": "4jDFCTBrd-PH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Logistic Regression\n",
        "2.   Decision Tree for Classification Problems\n",
        "3.   Random Forests for Classification Problems\n",
        "4.   K-Nearest Neighbor for Classification Problems\n",
        "\n",
        "including the following concept: *GridSearchCV for Hyperparameter Tuning*\n",
        "\n",
        "and measuring the accuracy and performance of the models with the following metrics:\n",
        "\n",
        "1.   Confusion Matrix\n",
        "2.   Accuracy Score\n",
        "3.   Precision\n",
        "4.   Recall\n",
        "5.   F1 Score\n",
        "\n",
        "And the lecture will end with the following concept: *Automating Workflows with Pipelines*"
      ],
      "metadata": {
        "id": "XYN6N5B-chWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Importing necessary libraries**\n",
        "\n"
      ],
      "metadata": {
        "id": "_yL0NdCEBHeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "pd.set_option(\"display.max_rows\",None)\n",
        "from sklearn import preprocessing\n",
        "import matplotlib\n",
        "matplotlib.style.use('ggplot')\n",
        "from sklearn.utils import shuffle\n",
        "pd.options.display.float_format = '{:.2f}'.format\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "PnEl3pSBBAHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Reading & Shuffling Data**"
      ],
      "metadata": {
        "id": "zzIiUwbE83Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('titanic.csv')\n",
        "df = df.drop(columns = [\"PassengerId\", \"Name\", \"Ticket\"], axis = 1)\n",
        "\n",
        "df = shuffle(df, random_state=42)\n",
        "\n",
        "print('Data shape: ', df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDGM5IkmBAEQ",
        "outputId": "05dbb893-7d06-4a3f-b2f0-7dc9d313e187"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape:  (891, 9)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Survived  Pclass     Sex   Age  SibSp  Parch  Fare Cabin Embarked\n",
              "709         1       3    male   NaN      1      1 15.25   NaN        C\n",
              "439         0       2    male 31.00      0      0 10.50   NaN        S\n",
              "840         0       3    male 20.00      0      0  7.92   NaN        S\n",
              "720         1       2  female  6.00      0      1 33.00   NaN        S\n",
              "39          1       3  female 14.00      1      0 11.24   NaN        C"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc0c52ce-d95f-453d-84f7-d38bdb3149a7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>709</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>15.25</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>439</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>male</td>\n",
              "      <td>31.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>10.50</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>840</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>male</td>\n",
              "      <td>20.00</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>7.92</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>720</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>female</td>\n",
              "      <td>6.00</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>33.00</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>female</td>\n",
              "      <td>14.00</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>11.24</td>\n",
              "      <td>NaN</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc0c52ce-d95f-453d-84f7-d38bdb3149a7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc0c52ce-d95f-453d-84f7-d38bdb3149a7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc0c52ce-d95f-453d-84f7-d38bdb3149a7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-0c7fc4dc-3f8c-484c-a47c-1dc544822c4b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0c7fc4dc-3f8c-484c-a47c-1dc544822c4b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-0c7fc4dc-3f8c-484c-a47c-1dc544822c4b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 891,\n  \"fields\": [\n    {\n      \"column\": \"Survived\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Pclass\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sex\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"female\",\n          \"male\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Age\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.526497332334042,\n        \"min\": 0.42,\n        \"max\": 80.0,\n        \"num_unique_values\": 88,\n        \"samples\": [\n          12.0,\n          31.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SibSp\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 8,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Parch\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Fare\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 49.6934285971809,\n        \"min\": 0.0,\n        \"max\": 512.3292,\n        \"num_unique_values\": 248,\n        \"samples\": [\n          8.1583,\n          7.75\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Cabin\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"A26\",\n          \"C22 C26\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Embarked\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"C\",\n          \"S\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Checking class balance for the \"Survived\" feature**"
      ],
      "metadata": {
        "id": "NPmucH5m9JaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['Survived'].value_counts())  # returns raw number\n",
        "print(df['Survived'].value_counts(normalize=True))  # returns percentages\n",
        "\n",
        "# the dataset is imbalanced: 62% to 38%.\n",
        "\n",
        "# Once we model the data, we will perform the following metrics:\n",
        "    # precision\n",
        "    # recall\n",
        "    # f1 score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHklro-0BABS",
        "outputId": "a4044ffe-6296-4c22-8015-b113554780ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    549\n",
            "1    342\n",
            "Name: Survived, dtype: int64\n",
            "0   0.62\n",
            "1   0.38\n",
            "Name: Survived, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Implementing Pre-Processing Steps**"
      ],
      "metadata": {
        "id": "8YykElpp9nH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Listing Missing Values\n",
        "2.   \"Cabin\" field is dropped\n",
        "3.   \"Embarked\" field is imputed with the mode value of the field\n",
        "4.   Parch and SibSp columns are combined under the \"Family\" field\n",
        "5.   Age column is imputed and converted into categorical data:"
      ],
      "metadata": {
        "id": "KtWKNIHnePRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_line(number, text):\n",
        "  print(f\"\\n{number}: {text}\")\n",
        "  print(40*\"-\")\n",
        "\n",
        "# 1. Listing Missing Values\n",
        "text_line(1, \"Missing Values\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 2. Cabin field is dropped\n",
        "df.drop(\"Cabin\",inplace=True,axis=1)\n",
        "text_line(2, \"Cabin field is dropped\")\n",
        "\n",
        "# 3. Embarked field is imputed with the mode value of the field\n",
        "df['Embarked'].fillna(df['Embarked'].mode()[0],inplace=True)\n",
        "text_line(3, \"Embarked field is imputed with the mode value\")\n",
        "\n",
        "# 4. Parch and SibSp columns are combined under the \"Family\" field\n",
        "def combine(df,col1,col2):\n",
        "    df[\"Family\"] = df[col1]+df[col2]\n",
        "    df.drop([col1,col2],inplace=True,axis=1)\n",
        "    return df\n",
        "df = combine(df,'SibSp','Parch')\n",
        "text_line(4, \"Parch and SibSp columns are combined under the Family field\")\n",
        "\n",
        "# 5. Age column is imputed and converted into categorical data, then Age field is dropped:\n",
        "    # -1 to 0       => Missing\n",
        "    #  0 to 5       => Infant\n",
        "    #  5 to 12      => Child\n",
        "    # 12 to 18      => Teenager\n",
        "    # 18 to 35      => Young Adult\n",
        "    # 35 to 60      => Adult\n",
        "    # 60 to 100     => Senior\n",
        "\n",
        "df[\"Age\"] = df[\"Age\"].fillna(-0.5)\n",
        "def process_age(df,cut_points,label_names):\n",
        "    df[\"Age_categories\"] = pd.cut(df[\"Age\"],cut_points,labels=label_names)\n",
        "    return df\n",
        "cut_points = [-1,0,5,12,18,35,60,100]\n",
        "label_names = [\"Missing\",\"Infant\",\"Child\",\"Teenager\",\"Young Adult\",\"Adult\",\"Senior\"]\n",
        "df = process_age(df,cut_points,label_names)\n",
        "text_line(5, \"Age column is imputed and converted into categorical data\")\n",
        "\n",
        "df.drop(\"Age\",inplace=True,axis=1)\n",
        "text_line(6, \"Age column is dropped\")\n",
        "\n",
        "text_line(7, \"Null in Training set\")\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tn_0tV-A_-l",
        "outputId": "28a51df0-20f4-4de3-fa74-9d6fcbb19bb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1: Missing Values\n",
            "----------------------------------------\n",
            "Survived      0\n",
            "Pclass        0\n",
            "Sex           0\n",
            "Age         177\n",
            "SibSp         0\n",
            "Parch         0\n",
            "Fare          0\n",
            "Cabin       687\n",
            "Embarked      2\n",
            "dtype: int64\n",
            "\n",
            "2: Cabin field is dropped\n",
            "----------------------------------------\n",
            "\n",
            "3: Embarked field is imputed with the mode value\n",
            "----------------------------------------\n",
            "\n",
            "4: Parch and SibSp columns are combined under the Family field\n",
            "----------------------------------------\n",
            "\n",
            "5: Age column is imputed and converted into categorical data\n",
            "----------------------------------------\n",
            "\n",
            "6: Null in Training set\n",
            "----------------------------------------\n",
            "Survived          0\n",
            "Pclass            0\n",
            "Sex               0\n",
            "Age               0\n",
            "Fare              0\n",
            "Embarked          0\n",
            "Family            0\n",
            "Age_categories    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Getting the Numerical and Categorical Feature groups ready for Feature Scaling**"
      ],
      "metadata": {
        "id": "9fFSvI-YEjVb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Features : Survived Age Fare Family\n",
        "# the scaling of target value is generally not required\n",
        "\n",
        "col = list(df.columns)\n",
        "categorical_features = df.select_dtypes(include = [\"object\"]).columns\n",
        "# Pclass and Age_categories are redefined as categorical data\n",
        "cat_features = categorical_features.append(pd.Index([\"Pclass\", \"Age_categories\"]))\n",
        "\n",
        "numerical_features_df = df.drop(columns = cat_features)\n",
        "numerical_features_df.drop(columns = [\"Survived\"], inplace=True)\n",
        "\n",
        "print('Categorical Features :',*cat_features)\n",
        "print('Numerical Features :',*numerical_features_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tj4Tm4UDHFK6",
        "outputId": "030b650b-557c-4b96-f649-7ca2e0447991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Categorical Features : Sex Embarked Pclass Age_categories\n",
            "Numerical Features : Age Fare Family\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Feature Scaling**\n"
      ],
      "metadata": {
        "id": "IJT6quwa27ik"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Robust Scaler\n",
        "*   MinMax Scaler\n",
        "*   Standard Scaler\n",
        "\n",
        "Note: implemented only for numerical fields\n",
        "<hr>\n",
        "\n",
        "##### Extra Information about ***Robust Scaler***\n",
        "\n",
        "When working with outliers we can use Robust Scaling for scaling our data,\n",
        "It scales features using statistics that are robust to outliers. This method removes the median and scales the data in the range between 1st quartile and 3rd quartile. i.e., in between 25th quantile and 75th quantile range. This range is also called an Interquartile range.\n",
        "The median and the interquartile range are then stored so that it could be used upon future data using the transform method. If outliers are present in the dataset, then the median and the interquartile range provide better results and outperform the sample mean and variance.\n",
        "RobustScaler uses the interquartile range so that it is robust to outliers"
      ],
      "metadata": {
        "id": "4HJ7Pw7MXK_V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Scaling is implemented only for numerical fields\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "\n",
        "scaler = preprocessing.RobustScaler()\n",
        "robust_df = scaler.fit_transform(numerical_features_df)\n",
        "robust_df = pd.DataFrame(robust_df, columns = numerical_features_df.columns)\n",
        "\n",
        "scaler = preprocessing.StandardScaler()\n",
        "standard_df = scaler.fit_transform(numerical_features_df)\n",
        "standard_df = pd.DataFrame(standard_df, columns = numerical_features_df.columns)\n",
        "\n",
        "scaler = preprocessing.MinMaxScaler()\n",
        "minmax_df = scaler.fit_transform(numerical_features_df)\n",
        "minmax_df = pd.DataFrame(minmax_df, columns = numerical_features_df.columns)\n",
        "\n",
        "fig, (ax1, ax2, ax3, ax4)  = plt.subplots(ncols = 4, figsize =(20, 5))\n",
        "\n",
        "ax1.set_title('Before Scaling')\n",
        "# sns.kdeplot(df['Age'], ax = ax1, color ='black')\n",
        "sns.kdeplot(df['Fare'], ax = ax1, color ='green')\n",
        "sns.kdeplot(df['Family'], ax = ax1, color ='red')\n",
        "\n",
        "ax2.set_title('After Robust Scaling')\n",
        "# sns.kdeplot(robust_df['Age'], ax = ax2, color ='black')\n",
        "sns.kdeplot(robust_df['Fare'], ax = ax2, color ='green')\n",
        "sns.kdeplot(robust_df['Family'], ax = ax2, color ='red')\n",
        "\n",
        "ax3.set_title('After Standard Scaling')\n",
        "# sns.kdeplot(standard_df['Age'], ax = ax3, color ='black')\n",
        "sns.kdeplot(standard_df['Fare'], ax = ax3, color ='green')\n",
        "sns.kdeplot(standard_df['Family'], ax = ax3, color ='red')\n",
        "\n",
        "ax4.set_title('After Min-Max Scaling')\n",
        "# sns.kdeplot(minmax_df['Age'], ax = ax4, color ='black')\n",
        "sns.kdeplot(minmax_df['Fare'], ax = ax4, color ='green')\n",
        "sns.kdeplot(minmax_df['Family'], ax = ax4, color ='red')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TbDB00F_hqBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[robust_df.columns] = robust_df.copy()"
      ],
      "metadata": {
        "id": "IO4KpxT58TXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Encoding Categorical Variables**"
      ],
      "metadata": {
        "id": "ONsi6nGjFy8y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   OneHotEncoder (this is what we are going to implement in the project)\n",
        "*   LabelEncoder\n",
        "\n",
        "Note: implemented only for categorical fields"
      ],
      "metadata": {
        "id": "YdimHahUXP6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding is only implemented for ... fields\n",
        "\n",
        "...\n",
        "encoder = OneHotEncoder(sparse = False, drop=\"first\")\n",
        "encoded_categories = encoder.fit_transform(df[cat_features]) # data\n",
        "encoded_feature_names = encoder.get_feature_names_out(cat_features) # feature names\n",
        "encoded_df = pd.DataFrame(encoded_categories, columns = encoded_feature_names) # creating a dataframe from the data with the feature names\n",
        "df = pd.concat([df.reset_index(drop=True), encoded_df.reset_index(drop=True)], axis = ...)\n",
        "df.drop(df[cat_features], inplace=True, axis = 1) # drop the original categorical features"
      ],
      "metadata": {
        "id": "KA7gzApShp-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Splitting Data**"
      ],
      "metadata": {
        "id": "-FsAJ-Ufa6Df"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "\n",
        "# split data into input objects and output object and perform the train test split process\n",
        "\n",
        "X = df.drop([\"Survive\"], axis = 1)\n",
        "y = df[\"Survived\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42, stratify =y)"
      ],
      "metadata": {
        "id": "7FC695_QXmxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Grid Search for Hyperparameter Tuning**"
      ],
      "metadata": {
        "id": "unjIni6wEdS9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Grid Search is a technique used to isolate the optimal hyperparameters of a model, which results in the most accurate predictions.\n",
        "\n",
        "Shortly, it is used to improve the accuracy of our model.<br>\n",
        "**What exactly are hyperparameters?**\n",
        "\n",
        "They are the model settings we apply before training our model.\n",
        "\n",
        "\n",
        "> **k** in KNN\n",
        "\n",
        "> **max depth** in Decision Trees\n",
        "\n",
        "> **Number of Trees** in Random Forest\n",
        "\n",
        "These are values that, as a data scientist, at most times, we do not know what value they should be. We are essentially guessing what it might be mostly based on our previous experience."
      ],
      "metadata": {
        "id": "w-veJaGMXDpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the functionalities for Grid Search\n",
        "..."
      ],
      "metadata": {
        "id": "34LPzSeIXpvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Best Evaluation Metric**"
      ],
      "metadata": {
        "id": "6TbRQQ2fDecP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Accuracy (for balanced data only)\n",
        "*   Precision (for imbalanced data)\n",
        "*   Recall (for imbalanced data)\n",
        "*   F1 Score (for imbalanced data)\n",
        "\n",
        "\n",
        "If we talk about classification problems, the most common metrics are:\n",
        "\n",
        "1.   **Balanced Data (at most 60%-40%, the gap more than that represents imbalanced)**\n",
        "\n",
        "  Consider metrics:\n",
        "\n",
        "  - Accuracy\n",
        "  - Area under the ROC (Receiver Operating Characteristic) curve or simply AUC (AUC)\n",
        "  \n",
        "  ROC curves are best used when the dataset is well-balanced. In other words, when the proportion of positive and negative classes are similar.\n",
        "  \n",
        "  In scenarios when data is imbalanced, ROC curve as well as accuracy score results can be misleading.\n",
        "\n",
        "---\n",
        "\n",
        "2.   **Imbalanced Data**\n",
        "\n",
        "  Consider metrics:\n",
        "\n",
        "  - Precision (P) (use Precision when the cost of False Positives is important)\n",
        "  - Recall (R) (use Recall when the cost of False Negatives is important.)\n",
        "  - F1 score (F1)  \n",
        "  (2 *Recall *Precision)/(Recall+Precision)\n",
        "\n",
        "  For cases with imbalanced data, we can consider precision, recall and f1 scores and find the probability threshold where f1 is maximized."
      ],
      "metadata": {
        "id": "tLSiGc2nEEri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning and Modelling Random Forest Classification Algorithm**"
      ],
      "metadata": {
        "id": "9vr5VvxUbuZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random forest is an ensemble model consisting of many decision trees working together across different randomly selected subsets of the data, facilitating improved accuracy and stability."
      ],
      "metadata": {
        "id": "_-UTU28O_F4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "param_grid_cv = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_sample_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'bootstrap': [True, False]\n",
        "    }\n",
        "\n",
        "\n",
        "# Explanations\n",
        "# n_estimators: The number of decision trees in the forest.\n",
        "# Increasing this value generally improves performance but also increases computation time.\n",
        "\n",
        "# max_depth: The maximum depth of each decision tree.\n",
        "# This controls how deeply each tree is allowed to grow during training and helps prevent overfitting.\n",
        "\n",
        "# min_samples_split: The minimum number of samples required to split an internal node.\n",
        "# Increasing this value helps prevent overfitting.\n",
        "\n",
        "# min_samples_leaf: The minimum number of samples required to be at a leaf node.\n",
        "# Increasing this value helps prevent overfitting by smoothing the model.\n",
        "\n",
        "# max_features: The number of features to consider when looking for the best split.\n",
        "# It can be a fixed value or a percentage of the total number of features.\n",
        "\n",
        "# bootstrap: Whether to bootstrap samples when building trees.\n",
        "# If set to True, each tree is built on a random subset of the training data with replacement.\n",
        "\n",
        "# remember that the more parameters you specify, the more parameter combinations of models it will need to built to train and test\n",
        "# this can end up with longer time\n",
        "\n",
        "# scoring = 'accuracy'\n",
        "#           'precision'\n",
        "#           'recall'\n",
        "#           'f1'\n",
        "#           'roc_auc'\n",
        "\n",
        "gs_cv = GridSearchCV(\n",
        "    estimator= RandomForestClassifier(random_state = 42),   # random_state=42 to get consistent result\n",
        "    param_grid = param_grid_cv,\n",
        "    cv = 5,                                                # cv is the number of the partitions that GridSearchCV will use for cross validation\n",
        "    scoring = \"precision\",                                 # it is a scoring metric to validate the model\n",
        "    n_jobs = -1,                                           # as this can be a computationally intensive process, we can specify another n_jobs = -1\n",
        "    refit = True)                                          # which means that it will use all our computer's processes to run the task and this can help speed things up\n",
        "\n",
        "\n",
        "# it might take a little bit of time as it will build, train and test the different combinations of the model\n",
        "gs_cv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "UyFZ2pYmbu0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to obtain the best/optimal parameters\n",
        "\n",
        "print(gs_cv.best_params_)\n",
        "\n",
        "# to get the best model from the grid search object without any further re-modelling\n",
        "best_model = gs_cv.best_estimator_\n",
        "best_model\n",
        "\n",
        "# remember that any parameter not included in the param_grid_cv will have their\n",
        "# default values as they are not included in the param_grid_cv dictionary\n",
        "\n",
        "# Predicting output\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "4YLSQxuwiyWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performance Measurement for Random Forest Classification Algorithm**"
      ],
      "metadata": {
        "id": "xHR0j3qGg1dW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "pFPWNrPYiGHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (10, 10))\n",
        "plt.style.use(\"seaborn-poster\")\n",
        "plt.matshow(conf_matrix, cmap=\"coolwarm\")\n",
        "plt.gca().xaxis.tick_bottom()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Class\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "print(f\"\\nConfusion Matrix: \\n {20*'-'}\\n{conf_matrix}\")\n",
        "\n",
        "for (i, j), corr_value in np.ndenumerate(conf_matrix):\n",
        "    plt.text(j, i, corr_value, ha=\"center\", va=\"center\", fontsize = 20) # ha:horizontal alignment, va:vertical alignment\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ewJ7IOZgzwbz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Accuracy Score**"
      ],
      "metadata": {
        "id": "q-DaLReQEB-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of correct classification out of all attempted classifications.\n",
        "print(f\"\\nAccuracy score of the model: {accuracy_score(y_test, y_pred)}\\n\")"
      ],
      "metadata": {
        "id": "wqcPcz_X2cIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Precision**"
      ],
      "metadata": {
        "id": "nNJnY242iWkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all observations that were predicted as positive, how many were actually positive?\n",
        "print(f\"\\nSpecificity of the model: {round(precision_score(y_test, y_pred), 2)}\\n\")"
      ],
      "metadata": {
        "id": "CGO1kB0K2sX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Recall**"
      ],
      "metadata": {
        "id": "qOocKC9SiZjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all actually positive observations, how many did we predict as positive?\n",
        "print(f\"\\nSensitivity of the model: {round(recall_score(y_test, y_pred), 2)}\\n\")"
      ],
      "metadata": {
        "id": "McCHuNw36ms7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **F1 Score**"
      ],
      "metadata": {
        "id": "UFQiHivyicj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nF1 score of the model: {round(f1_score(y_test, y_pred, zero_division=0), 2)}\\n\")"
      ],
      "metadata": {
        "id": "GuyUKGou6nZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning and Modelling Logistic Regression Classification Algorithm**"
      ],
      "metadata": {
        "id": "PDOq76cZ0jDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the hyperparameters to search\n",
        "param_grid_cv = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.01, 0.1, 1.0, 10.0],\n",
        "    'solver': ['lilinear', 'lbfgs', 'saga'],\n",
        "    'max_iter': [100, 200, 300],\n",
        "    'class_weight': [None, 'balanced'],\n",
        "    'random_state': [42]\n",
        "}\n",
        "\n",
        "# Explanations\n",
        "# penalty: Determines the regularization term to apply.\n",
        "# It can be either 'l1' for L1 regularization (Lasso) or 'l2' for L2 regularization (Ridge).\n",
        "# Regularization helps prevent overfitting by penalizing large coefficients.\n",
        "\n",
        "# C: Inverse of regularization strength.\n",
        "# Smaller values specify stronger regularization.\n",
        "# It's the inverse of lambda in regularization equations.\n",
        "# Higher values of C specify less regularization, which can lead to overfitting if the model complexity is not controlled.\n",
        "\n",
        "# solver: Algorithm to use in optimization problem.\n",
        "# The choice of solver depends on the size and structure of the data.\n",
        "# For smaller datasets, 'liblinear' is usually a good choice.\n",
        "# For larger datasets, 'lbfgs', 'sag', or 'saga' might be more appropriate.\n",
        "\n",
        "# max_iter: Maximum number of iterations taken for the solvers to converge.\n",
        "\n",
        "# tol: Tolerance for stopping criteria.\n",
        "\n",
        "# class_weight: Weights associated with classes.\n",
        "# Useful for handling imbalanced datasets.\n",
        "# Options include 'balanced', which automatically adjusts weights inversely proportional to class frequencies, or you can specify your own weights.\n",
        "\n",
        "# random_state: Seed for random number generation. It ensures reproducibility of results.\n",
        "\n",
        "# remember that the more parameters you specify, the more parameter combinations of models it will need to built to train and test\n",
        "# this can end up with longer time\n",
        "\n",
        "# scoring = 'accuracy'\n",
        "#           'precision'\n",
        "#           'recall'\n",
        "#           'f1'\n",
        "#           'roc_auc'\n",
        "\n",
        "\n",
        "gs_cv = GridSearchCV(\n",
        "    estimator= LogisticRegression(random_state = 42),   # random_state=42 to get consistent result\n",
        "    param_grid = param_grid_cv,\n",
        "    cv = 5,                                                # cv is the number of the partitions that GridSearchCV will use for cross validation\n",
        "    scoring = \"recall\",                                    # it is a scoring metric to validate the model\n",
        "    n_jobs = -1,                                           # as this can be a computationally intensive process, we can specify another n_jobs = -1\n",
        "    refit = True)                                          # which means that it will use all our computer's processes to run the task and this can help speed things up\n",
        "\n",
        "\n",
        "# it might take a little bit of time as it will build, train and test the different combinations of the model\n",
        "gs_cv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7BT1eYldA_71"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to obtain the best/optimal parameters\n",
        "\n",
        "print(gs_cv.best_params_)\n",
        "\n",
        "# to get the best model from the grid search object without any further re-modelling\n",
        "best_model = gs_cv.best_estimator_\n",
        "best_model\n",
        "\n",
        "# remember that any parameter not included in the param_grid_cv will have their\n",
        "# default values as they are not included in the param_grid_cv dictionary\n",
        "\n",
        "# Predicting output\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Rg2j-g6_OLlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performance Measurement for Logistic Regression Classification Algorithm**"
      ],
      "metadata": {
        "id": "ZBYyru9n2Yc2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Confusion Matrix**\n",
        "\n"
      ],
      "metadata": {
        "id": "GKEvdUlqEaZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.style.use(\"seaborn-poster\")\n",
        "plt.matshow(conf_matrix, cmap=\"coolwarm\")\n",
        "plt.gca().xaxis.tick_bottom()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Class\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "print(f\"\\nConfusion Matrix: \\n {20*'-'}\\n{conf_matrix}\")\n",
        "\n",
        "for (i, j), corr_value in np.ndenumerate(conf_matrix):\n",
        "    print(f\"column-row: {j} {i}, value: {corr_value}\")\n",
        "    plt.text(j, i, corr_value, ha=\"center\", va=\"center\", fontsize = 20) # ha:horizontal alignment, va:vertical alignment\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3BGwCAYpCoOX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Accuracy**"
      ],
      "metadata": {
        "id": "XFoCu7bjBtcw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of correct classification out of all attempted classifications.\n",
        "print(f\"\\nAccuracy score of the model: {accuracy_score(y_test, y_pred)}\\n\")"
      ],
      "metadata": {
        "id": "m13Ql6_qBvaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Precision**"
      ],
      "metadata": {
        "id": "tbBeYn4eEymW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all observations that were predicted as positive, how many were actually positive?\n",
        "print(f\"\\nPrecision score of the model: {round(precision_score(y_test, y_pred), 2)}\\n\")\n",
        "\n",
        "# each time we predicted positive class, we were corrected by 75%"
      ],
      "metadata": {
        "id": "L3yso6MCCoLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Recall**"
      ],
      "metadata": {
        "id": "0pyK5_ORFALZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all actually positive observations, how many did we predict as positive?\n",
        "print(f\"\\nRecall score of the model: {round(recall_score(y_test, y_pred), 2)}\\n\")"
      ],
      "metadata": {
        "id": "_9lIYvZqCkBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **F1 Score**"
      ],
      "metadata": {
        "id": "Y1-y2bSeFQYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nF1 score of the model: {round(f1_score(y_test, y_pred, zero_division=0), 2)}\\n\")\n"
      ],
      "metadata": {
        "id": "xpNiIt2b3WoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning and Modelling Decision Tree Classification Algorithm**"
      ],
      "metadata": {
        "id": "MHIkaDxpCUUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your turn\n",
        "...\n",
        "\n",
        "# Explanations\n",
        "# Criterion: This hyperparameter measures the quality of a split.\n",
        "# Typical options are \"gini\" for the Gini impurity and \"entropy\" for information gain.\n",
        "\n",
        "# Max Depth: It specifies the maximum depth of the tree.\n",
        "# A deeper tree can capture more complex relationships in the data but may also lead to overfitting.\n",
        "\n",
        "# Min Samples Split: The minimum number of samples required to split an internal node.\n",
        "# Increasing this value can prevent overfitting by ensuring that each node has enough samples to split.\n",
        "\n",
        "# Min Samples Leaf: The minimum number of samples required to be at a leaf node.\n",
        "# It prevents the tree from creating nodes with very few samples, which can lead to overfitting.\n",
        "\n",
        "# Max Features: The number of features to consider when looking for the best split.\n",
        "# It helps to reduce the number of features considered, which can speed up the training process and reduce overfitting.\n",
        "\n",
        "..."
      ],
      "metadata": {
        "id": "X2kcBGjS9bgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to obtain the best/optimal parameters\n",
        "\n",
        "print(gs_cv.best_params_)\n",
        "\n",
        "# to get the best model from the grid search object without any further re-modelling\n",
        "best_model = gs_cv.best_estimator_\n",
        "best_model\n",
        "\n",
        "# remember that any parameter not included in the param_grid_cv will have their\n",
        "# default values as they are not included in the param_grid_cv dictionary\n",
        "\n",
        "# Predicting output\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "GzgfsNzkOSNp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performance Measurement for Decision Tree Classification Algorithm**"
      ],
      "metadata": {
        "id": "hTGywT588J14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "6TKpajTYKhzG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (10, 10))\n",
        "plt.style.use(\"seaborn-poster\")\n",
        "plt.matshow(conf_matrix, cmap=\"coolwarm\")\n",
        "plt.gca().xaxis.tick_bottom()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Class\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "print(f\"\\nConfusion Matrix: \\n {20*'-'}\\n{conf_matrix}\")\n",
        "\n",
        "for (i, j), corr_value in np.ndenumerate(conf_matrix):\n",
        "    plt.text(j, i, corr_value, ha=\"center\", va=\"center\", fontsize = 20) # ha:horizontal alignment, va:vertical alignment\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VxAtycOXJM2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Accuracy Score**"
      ],
      "metadata": {
        "id": "GlsCKui0KulA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of correct classification out of all attempted classifications.\n",
        "print(f\"\\nAccuracy score of the model: {accuracy_score(y_test, y_pred)}\\n\")"
      ],
      "metadata": {
        "id": "PNN2pyqwJM0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Precision**"
      ],
      "metadata": {
        "id": "uQTnOOdKK1Au"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all observations that were predicted as positive, how many were actually positive?\n",
        "print(f\"\\nPrecision score of the model: {round(precision_score(y_test, y_pred), 2)}\\n\")\n",
        "\n",
        "# each time we predicted positive class, we were corrected by 93%"
      ],
      "metadata": {
        "id": "q7KIu75uJMxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Recall**"
      ],
      "metadata": {
        "id": "bK_v5_ioLBPo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all actually positive observations, how many did we predict as positive?\n",
        "print(f\"\\nRecall score of the model: {round(recall_score(y_test, y_pred), 2)}\\n\")"
      ],
      "metadata": {
        "id": "IHHWFYNE9bdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **F1 Score**"
      ],
      "metadata": {
        "id": "fP5EL2z8v3RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test, y_pred, zero_division=0)"
      ],
      "metadata": {
        "id": "6eRKgMraZOKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Hyperparameter Tuning and Modelling KNN Classification Algorithm**"
      ],
      "metadata": {
        "id": "QHIJQvMu03u8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The KNN algorithm predicts a class for an unknown data point using the most popular class of a number of nearby known data points.\n",
        "\n",
        "The number of nearby data points used to form the prediction is denoted by k.\n",
        "\n",
        "Make sure you handle outliers when performing this algorithm as this is a distance-based model. Outliers can cause issues particularly when scaling the data."
      ],
      "metadata": {
        "id": "5V4RTMO-_fZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# Define the hyperparameters to search\n",
        "param_grid_cv = {\n",
        "    ...\n",
        "}\n",
        "\n",
        "# Explanations\n",
        "# n_neighbors: Number of neighbors to consider.\n",
        "# It's a crucial hyperparameter that controls the model complexity.\n",
        "# Higher values lead to smoother decision boundaries but can result in underfitting, while lower values can lead to overfitting.\n",
        "\n",
        "# weights: Determines how the neighbors are weighted.\n",
        "# Possible options are 'uniform' (all neighbors are weighted equally) and 'distance' (closer neighbors have more influence).\n",
        "\n",
        "# algorithm: Specifies the algorithm used to compute the nearest neighbors.\n",
        "# Options include 'auto', 'ball_tree', 'kd_tree', and 'brute'.\n",
        "# The 'auto' option automatically selects the most appropriate algorithm based on the input data.\n",
        "\n",
        "# metric: The distance metric used to measure the distance between instances.\n",
        "# Common options include 'euclidean', 'manhattan', and 'minkowski'.\n",
        "\n",
        "# leaf_size: Leaf size passed to BallTree or KDTree.\n",
        "# It affects the speed of the construction and query but may not have a significant impact on the quality of the model.\n",
        "\n",
        "# remember that the more parameters you specify, the more parameter combinations of models it will need to built to train and test\n",
        "# this can end up with longer time\n",
        "\n",
        "# scoring = 'accuracy'\n",
        "#           'precision'\n",
        "#           'recall'\n",
        "#           'f1'\n",
        "#           'roc_auc'\n",
        "\n",
        "gs_cv = GridSearchCV(\n",
        "    ...\n",
        "    )\n",
        "\n",
        "\n",
        "# it might take a little bit of time as it will build, train and test the different combinations of the model\n",
        "gs_cv.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "y36dFrGK1aX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# to obtain the best/optimal parameters\n",
        "\n",
        "print(gs_cv.best_params_)\n",
        "\n",
        "# to get the best model from the grid search object without any further re-modelling\n",
        "best_model = gs_cv.best_estimator_\n",
        "best_model\n",
        "\n",
        "# remember that any parameter not included in the param_grid_cv will have their\n",
        "# default values as they are not included in the param_grid_cv dictionary\n",
        "\n",
        "# Predicting output\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "WXPLhbSoOVv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Performance Measurement for KNN Classification Algorithm**"
      ],
      "metadata": {
        "id": "DoJhq-k9_0v-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Confusion Matrix**"
      ],
      "metadata": {
        "id": "JelB4trDZsQD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize = (10, 10))\n",
        "plt.style.use(\"seaborn-poster\")\n",
        "plt.matshow(conf_matrix, cmap=\"coolwarm\")\n",
        "plt.gca().xaxis.tick_bottom()\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.ylabel(\"Actual Class\")\n",
        "plt.xlabel(\"Predicted Class\")\n",
        "print(f\"\\nConfusion Matrix: \\n {20*'-'}\\n{conf_matrix}\")\n",
        "\n",
        "for (i, j), corr_value in np.ndenumerate(conf_matrix):\n",
        "    print(f\"column-row: {j} {i}, value: {corr_value}\")\n",
        "    plt.text(j, i, corr_value, ha=\"center\", va=\"center\", fontsize = 20) # ha:horizontal alignment, va:vertical alignment\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mi1rYoge1aVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Accuracy**"
      ],
      "metadata": {
        "id": "hOE5pLNKCqrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of correct classification out of all attempted classifications.\n",
        "print(f\"\\nAccuracy score of the model: {accuracy_score(y_test, y_pred)}\\n\")"
      ],
      "metadata": {
        "id": "Mvu7paYfCzx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Precision**"
      ],
      "metadata": {
        "id": "nYGUdyE0aIdi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all observations that were predicted as positive, how many were actually positive?\n",
        "print(f\"\\nPrecision score of the model: {round(precision_score(y_test, y_pred), 2)}\\n\")"
      ],
      "metadata": {
        "id": "5GJDpLf9aOXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Recall**"
      ],
      "metadata": {
        "id": "acGATcBOaQ_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# of all actually positive observations, how many did we predict as positive?\n",
        "print(f\"\\nRecall score of the model: {round(recall_score(y_test, y_pred), 2)}\\n\")"
      ],
      "metadata": {
        "id": "QqbeUbqzaR7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **F1 Score**"
      ],
      "metadata": {
        "id": "Mm0FLxofaabg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f1_score(y_test, y_pred, zero_division=0)"
      ],
      "metadata": {
        "id": "3fW3U1aoafEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Automating Workflows with Pipelines**"
      ],
      "metadata": {
        "id": "-mCX-9IIUoDM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pipelines are great for keeping the workflow from start to finish clean and effective.\n",
        "# so far, we have kept each step in the process separate including data preparation step where\n",
        "# we handled different tasks such as handling missing values, implemening encoding and so on.\n",
        "\n",
        "# pipelines tie all these steps together.\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer"
      ],
      "metadata": {
        "id": "mssg6PBsU6iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup Pipelines**\n",
        "\n",
        "We will go through a very small dataset where we will be able to apply a lot of column transofrmations. This will be a dataset for demonstration purposes only.\n",
        "\n",
        "Once you are comfortable with the Pipeline concept, we will perform the Pipeline object in our datasets that we have seen so far."
      ],
      "metadata": {
        "id": "CmzD7XqYtE2I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reading Data\n",
        "\n",
        "df = pd.read_csv('pipeline_data.csv')\n",
        "\n",
        "df = shuffle(df, random_state=42)\n",
        "\n",
        "print('Data shape: ', df.shape)\n",
        "\n",
        "print(df.isna().sum())\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Ynj1kewRxgQ2",
        "outputId": "e37d2062-3173-4f3a-cfcf-0bf1a24b02db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape:  (100, 4)\n",
            "purchase        0\n",
            "age             3\n",
            "gender          3\n",
            "credit_score    5\n",
            "dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    purchase   age gender  credit_score\n",
              "83         1 34.00      F        161.00\n",
              "53         1 35.00      M        318.00\n",
              "70         1 26.00      M        123.00\n",
              "45         0 41.00      F        517.00\n",
              "44         1 26.00      M        250.00"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8602261e-fccb-44ee-a11e-0a06440a89aa\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>purchase</th>\n",
              "      <th>age</th>\n",
              "      <th>gender</th>\n",
              "      <th>credit_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>1</td>\n",
              "      <td>34.00</td>\n",
              "      <td>F</td>\n",
              "      <td>161.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53</th>\n",
              "      <td>1</td>\n",
              "      <td>35.00</td>\n",
              "      <td>M</td>\n",
              "      <td>318.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>1</td>\n",
              "      <td>26.00</td>\n",
              "      <td>M</td>\n",
              "      <td>123.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>0</td>\n",
              "      <td>41.00</td>\n",
              "      <td>F</td>\n",
              "      <td>517.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>1</td>\n",
              "      <td>26.00</td>\n",
              "      <td>M</td>\n",
              "      <td>250.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8602261e-fccb-44ee-a11e-0a06440a89aa')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8602261e-fccb-44ee-a11e-0a06440a89aa button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8602261e-fccb-44ee-a11e-0a06440a89aa');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bae67064-599e-42d7-af9f-4c19dfe53b45\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bae67064-599e-42d7-af9f-4c19dfe53b45')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bae67064-599e-42d7-af9f-4c19dfe53b45 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()\n",
        "\n",
        "# point out the data type of the columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miAT7Sc80NJb",
        "outputId": "9e5c91c6-8a97-4824-f3cb-737d827ae68e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100 entries, 83 to 51\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   purchase      100 non-null    int64  \n",
            " 1   age           97 non-null     float64\n",
            " 2   gender        97 non-null     object \n",
            " 3   credit_score  95 non-null     float64\n",
            "dtypes: float64(2), int64(1), object(1)\n",
            "memory usage: 3.9+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the dataset has both numerical and categorical columns\n",
        "# we will apply null value imputation with the SimpleImputer for both categorical and numerical variables,\n",
        "# one hot encoding with the OneHotEncoder, scaling for the numerical fields with StandardScaler\n",
        "\n",
        "# lets first split the data:\n",
        "X=df.drop([\"purchase\"], axis = 1)\n",
        "y = df[\"purchase\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "metadata": {
        "id": "9aLoDjJNyruR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we will specify the numerical and categorical features in a separate list each\n",
        "\n",
        "numeric_features = [\"age\", \"credit_score\"]\n",
        "categorical_features = [\"gender\"]"
      ],
      "metadata": {
        "id": "iP33UzR_z-0T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we are now ready to setup some preprocessing pipelines"
      ],
      "metadata": {
        "id": "Be7El8Zl0j-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# numeric feature pipeline - scaling\n",
        "\n",
        "numeric_transformer = Pipeline(steps = [(\"imputer\", SimpleImputer()),\n",
        "                                        (\"scaler\", StandardScaler())])"
      ],
      "metadata": {
        "id": "BUWMs-B8sKJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical feature transformer\n",
        "\n",
        "# since the column is categorical, we need to specify the imputation strategy\n",
        "categorical_transformer = Pipeline(steps = [(\"imputer\", SimpleImputer(strategy = \"constant\", fill_value = \"U\")),(\"ohe\", OneHotEncoder())])"
      ],
      "metadata": {
        "id": "Hd4D2pV7sY4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once we run the pipelines, we are ready to pass those objects into one, overall object\n",
        "# we will use the column transformer functionality\n",
        "\n",
        "preprocessing_pipeline = ColumnTransformer(transformers = [(\"numeric\", numeric_transformer, numeric_features),\n",
        "                                                           (\"categorical\", categorical_transformer, categorical_features)])"
      ],
      "metadata": {
        "id": "NcWIXgEkw590"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we have so far identified the pipelines, but we have not applied them yet\n",
        "# now, we are ready to apply them\n",
        "# we will start with the logistic regression"
      ],
      "metadata": {
        "id": "6Yt7Abgw2q5i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets now create a pipeline where we will instantiate the Logistic Regression classifier\n",
        "\n",
        "# run it\n",
        "classifier = Pipeline(steps = [(\"preprocessing_pipeline\", preprocessing_pipeline),\n",
        "                                   (\"classifier\", LogisticRegression(random_state = 42))])\n",
        "\n",
        "\n",
        "# and now that we have created the classifier object, we can fit the model\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# you can see what exactly has happened when data pass the pipeline\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "rKfFMtaV3Dkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#lets now create a pipeline where we will instantiate the Random Forest classifier\n",
        "\n",
        "# run it\n",
        "classifier = Pipeline(steps = [(\"preprocessing_pipeline\", preprocessing_pipeline),\n",
        "                                   (\"classifier\", RandomForestClassifier(random_state = 42))])\n",
        "\n",
        "# and now that we have created the classifier object, we can fit the model\n",
        "\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# you can see what exactly has happened when data pass the pipeline\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCCfhOoM5NEB",
        "outputId": "c933deab-6a24-4221-be43-d6b12d51893b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# as you can see, we are passing unprocessed data\n",
        "# we could pass any new and unprocessed data as well"
      ],
      "metadata": {
        "id": "_0ozisy_5jPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we can save the pipeline for future use\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(classifier, \"model.joblib\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "id": "QT27Hd8P5aQW",
        "outputId": "4a20f3e5-0225-4754-e175-8f9a0afd739b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f164e1f7d2ab>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model.joblib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'classifier' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# restart kernel"
      ],
      "metadata": {
        "id": "S6g6mK0N6vdh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uIpndj_5V_v0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets import the pipeline as an object to the environment\n",
        "\n",
        "model = joblib.load(\"model.joblib\")"
      ],
      "metadata": {
        "id": "HdqKvq5KWmvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# once the model is loaded, we can create a brand new dataset and apply the pipeline on this unprocessed dataset\n",
        "\n",
        "new_df = pd.DataFrame({\"age\": [25, np.nan, 50],\n",
        "                       \"gender\": [\"M\", \"F\", np.nan],\n",
        "                       \"credit_score\": [200, 100, 500]})"
      ],
      "metadata": {
        "id": "XyQQemvAA59V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# now, lets pass the data in and receive predictions\n",
        "\n",
        "model.predict(new_df)"
      ],
      "metadata": {
        "id": "NDnY1_cVBlku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}